{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d1359-e4ee-4021-a128-fb829748ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "class EpisodeMetrics:\n",
    "    def __init__(self):\n",
    "        self.waiting_times = []\n",
    "        self.queue_lengths = []\n",
    "        self.num_vehicles = []\n",
    "        self.rewards = []\n",
    "        self.unique_vehicles = set()\n",
    "        self.exit_vehicles = set()\n",
    "\n",
    "    def update(self, waiting_time, queue_length, vehicles, reward):\n",
    "        self.waiting_times.append(waiting_time)\n",
    "        self.queue_lengths.append(queue_length)\n",
    "        self.num_vehicles.append(len(vehicles))\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "        # Identify newly exited vehicles\n",
    "        exited_vehicles = self.unique_vehicles - set(vehicles)\n",
    "        self.exit_vehicles.update(exited_vehicles)\n",
    "\n",
    "        # Update the unique vehicle set\n",
    "        self.unique_vehicles.update(vehicles)\n",
    "\n",
    "    def get_stats(self):\n",
    "        if not self.waiting_times:\n",
    "            return None\n",
    "        return {\n",
    "            'avg_waiting_time': np.mean(self.waiting_times),\n",
    "            'avg_queue_length': np.mean(self.queue_lengths),\n",
    "            'total_throughput': len(self.exit_vehicles),\n",
    "            'avg_reward': np.mean(self.rewards) if self.rewards else 0.0,\n",
    "            'total_vehicles_served': len(self.exit_vehicles),\n",
    "            'max_queue_length': max(self.queue_lengths),\n",
    "            'throughput_rate': len(self.exit_vehicles) / max(len(self.waiting_times), 1),\n",
    "            'avg_completion_time': np.mean([t for t in self.waiting_times if t > 0]),\n",
    "            'queue_stability': np.std(self.queue_lengths),\n",
    "            'system_efficiency': len(self.exit_vehicles) / max(sum(self.queue_lengths), 1)\n",
    "        }\n",
    "\n",
    "def evaluate_ppo_model(iterations=5):\n",
    "    print(\"Starting evaluation...\")\n",
    "    episode1_stats = []\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        print(f\"\\nIteration {iteration + 1}/{iterations}\")\n",
    "        \n",
    "        env = DummyVecEnv([lambda: TrafficLightEnv(Config.SUMO_CONFIG_FILE)])\n",
    "        metrics = EpisodeMetrics()\n",
    "        \n",
    "        try:\n",
    "            model = PPO.load(Config.MODEL_SAVE_PATH, env=env)\n",
    "            if iteration == 0:\n",
    "                print(\"Loaded trained model successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            return None, None\n",
    "        \n",
    "        obs = env.reset()\n",
    "        base_env = env.envs[0]\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        last_stats_before_step50 = None\n",
    "        \n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            for _ in range(Config.SIMULATION_STEPS):\n",
    "                obs, rewards, dones, info = env.step(action)\n",
    "                step_count += 1\n",
    "                \n",
    "                waiting_time, queue_length, vehicles = base_env._get_traffic_metrics()\n",
    "                metrics.update(waiting_time, queue_length, vehicles, rewards[0])\n",
    "                \n",
    "                print(f\"Step {step_count}: Vehicles={len(vehicles)}, Queue={queue_length}, \"\n",
    "                      f\"Waiting={waiting_time:.2f}, Reward={rewards[0]:.2f}\")\n",
    "                \n",
    "                if step_count == 49:  # Save stats before Step 50\n",
    "                    last_stats_before_step50 = metrics.get_stats()\n",
    "                \n",
    "                done = dones[0]\n",
    "                if done:\n",
    "                    break\n",
    "        \n",
    "        if last_stats_before_step50:\n",
    "            print(f\"Episode 1 Stats: {last_stats_before_step50}\")\n",
    "            episode1_stats.append(last_stats_before_step50)\n",
    "        \n",
    "        env.close()\n",
    "    \n",
    "    # Calculate final statistics using stats from before Step 50\n",
    "    if episode1_stats:\n",
    "        overall_stats = {\n",
    "            'avg_waiting_time': np.mean([stat['avg_waiting_time'] for stat in episode1_stats]),\n",
    "            'avg_queue_length': np.mean([stat['avg_queue_length'] for stat in episode1_stats]),\n",
    "            'avg_throughput': np.mean([stat['total_throughput'] for stat in episode1_stats]),\n",
    "            'avg_reward': np.mean([stat['avg_reward'] for stat in episode1_stats])\n",
    "        }\n",
    "        \n",
    "        overall_stats_std = {\n",
    "            'waiting_time_std': np.std([stat['avg_waiting_time'] for stat in episode1_stats]),\n",
    "            'queue_length_std': np.std([stat['avg_queue_length'] for stat in episode1_stats]),\n",
    "            'throughput_std': np.std([stat['total_throughput'] for stat in episode1_stats]),\n",
    "            'reward_std': np.std([stat['avg_reward'] for stat in episode1_stats])\n",
    "        }\n",
    "        \n",
    "        print(\"\\nOverall Performance:\")\n",
    "        for metric, value in overall_stats.items():\n",
    "            print(f\"  {metric}: {value:.2f} Â± {overall_stats_std[metric.replace('avg_', '') + '_std']:.2f}\")\n",
    "        \n",
    "        return overall_stats, overall_stats_std\n",
    "    else:\n",
    "        print(\"\\nNo valid statistics collected during evaluation\")\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_metrics, std_metrics = evaluate_ppo_model(iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88466706-fd63-4f34-8d0c-66256ed86646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gym\n",
    "import traci\n",
    "import torch\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "class Config:\n",
    "    LANE_NAMES = {\n",
    "        'NORTH': ['N_0', 'N_1'],\n",
    "        'EAST': ['E_0', 'E_1'],\n",
    "        'SOUTH': ['S_0', 'S_1'],\n",
    "        'WEST': ['W_0', 'W_1']\n",
    "    }\n",
    "    SUMO_CONFIG_FILE = \"cross/cross.sumocfg\"\n",
    "    MODEL_SAVE_PATH = \"ppo_traffic_model\"\n",
    "    MAX_STEPS = 500\n",
    "    TRAIN_TIMESTEPS = 10000\n",
    "    LEARNING_RATE = 1e-4\n",
    "    N_STEPS = 512\n",
    "    BATCH_SIZE = 64\n",
    "    N_EPOCHS = 10\n",
    "    SIMULATION_STEPS = 10\n",
    "    REWARD_WEIGHTS = {\n",
    "        'waiting_time': 0.4,\n",
    "        'queue_length': 0.5,\n",
    "        'emergency': 0.8,\n",
    "        'throughput': 0.3\n",
    "    }\n",
    "    TL_ID = \"0\"\n",
    "    NUM_PHASES = 4\n",
    "    VEHICLE_TYPES = ['passenger', 'truck', 'bus', 'emergency']\n",
    "\n",
    "class MetricsTracker:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.episode_metrics = defaultdict(list)\n",
    "        self.current_vehicles = set()  # Currently active vehicles\n",
    "        self.completed_vehicles = set()  # Vehicles that have exited\n",
    "        self.total_throughput = 0  # Track completed vehicles\n",
    "        self.episode_metrics['reward_components'] = [] \n",
    "        \n",
    "    def update(self, waiting_time, queue_length, vehicles_in_system, reward):\n",
    "        if reward is None:\n",
    "            reward = 0.0\n",
    "        \n",
    "        # Track basic metrics\n",
    "        self.episode_metrics['waiting_times'].append(waiting_time)\n",
    "        self.episode_metrics['queue_lengths'].append(queue_length)\n",
    "        \n",
    "        # Find newly completed vehicles (vehicles that were present but are now gone)\n",
    "        completed = self.current_vehicles - set(vehicles_in_system)\n",
    "        self.completed_vehicles.update(completed)\n",
    "        self.total_throughput += len(completed)  # Increment throughput by completed vehicles\n",
    "        \n",
    "        # Update current vehicles\n",
    "        self.current_vehicles = set(vehicles_in_system)\n",
    "        \n",
    "        # Store metrics\n",
    "        self.episode_metrics['throughput'].append(len(completed))  # Store incremental throughput\n",
    "        self.episode_metrics['rewards'].append(reward)\n",
    "        \n",
    "    def get_episode_stats(self):\n",
    "        stats = {\n",
    "            'avg_waiting_time': np.mean(self.episode_metrics['waiting_times']),\n",
    "            'avg_queue_length': np.mean(self.episode_metrics['queue_lengths']),\n",
    "            'total_throughput': len(self.completed_vehicles),  # Use total completed vehicles\n",
    "            'avg_reward': np.mean(self.episode_metrics['rewards']) if self.episode_metrics['rewards'] else 0.0,\n",
    "            'total_vehicles_served': len(self.completed_vehicles),\n",
    "            'max_queue_length': max(self.episode_metrics['queue_lengths'], default=0),\n",
    "            'throughput_rate': len(self.completed_vehicles) / max(len(self.episode_metrics['waiting_times']), 1),\n",
    "            'avg_completion_time': np.mean([t for t in self.episode_metrics['waiting_times'] if t > 0]),\n",
    "            'queue_stability': np.std(self.episode_metrics['queue_lengths']),\n",
    "            'system_efficiency': len(self.completed_vehicles) / max(sum(self.episode_metrics['queue_lengths']), 1)\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TrafficLightEnv(gym.Env):\n",
    "    def __init__(self, sumo_config):\n",
    "        super().__init__()\n",
    "        self.sumo_config = sumo_config\n",
    "        num_features = len(Config.VEHICLE_TYPES) + 4 + 4 + 1\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=float('inf'), shape=(num_features,), dtype=np.float32\n",
    "        )\n",
    "        self.action_space = spaces.Discrete(Config.NUM_PHASES)\n",
    "        self.current_step = 0\n",
    "        self.metrics = MetricsTracker()\n",
    "        self.connection_label = f\"conn_{random.randint(0, 10000)}\"\n",
    "        self.episode_metrics = {'reward_components': []}  # Initialize episode_metrics\n",
    "        # Initialize tracking variables for reward calculation\n",
    "        self._last_completed_count = 0\n",
    "        self._last_queue_length = 0\n",
    "\n",
    "    def _get_lane_direction(self, lane_id):\n",
    "        for direction, lanes in Config.LANE_NAMES.items():\n",
    "            if any(lane_id.startswith(lane) for lane in lanes):\n",
    "                return list(Config.LANE_NAMES.keys()).index(direction)\n",
    "        return None\n",
    "\n",
    "    def _get_traffic_metrics(self):\n",
    "        total_waiting_time, queue_length, vehicles_in_system = 0, 0, []\n",
    "        for lane_id in traci.lane.getIDList():\n",
    "            if lane_id.startswith(\":\"):\n",
    "                continue\n",
    "            waiting_time = traci.lane.getWaitingTime(lane_id)\n",
    "            total_waiting_time += waiting_time\n",
    "            vehicles = traci.lane.getLastStepVehicleIDs(lane_id)\n",
    "            queue_vehicles = [v for v in vehicles if traci.vehicle.getSpeed(v) < 0.1]\n",
    "            queue_length += len(queue_vehicles)\n",
    "            vehicles_in_system.extend(vehicles)\n",
    "        return total_waiting_time, queue_length, vehicles_in_system\n",
    "\n",
    "    def _calculate_reward(self, state):\n",
    "        \"\"\"\n",
    "        Calculate reward based on multiple traffic management objectives:\n",
    "        - Minimize waiting time\n",
    "        - Minimize queue length\n",
    "        - Maximize throughput\n",
    "        - Prioritize emergency vehicles\n",
    "        - Maintain traffic flow efficiency\n",
    "        \n",
    "        Args:\n",
    "            state: Current state observation containing traffic metrics\n",
    "            \n",
    "        Returns:\n",
    "            float: Calculated reward value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get current traffic metrics\n",
    "            waiting_time, queue_length, vehicles = self._get_traffic_metrics()\n",
    "            \n",
    "            # 1. Throughput Component\n",
    "            # Count newly completed vehicles since last step\n",
    "            completed_vehicles = len(self.metrics.completed_vehicles) - getattr(self, '_last_completed_count', 0)\n",
    "            self._last_completed_count = len(self.metrics.completed_vehicles)\n",
    "            throughput_reward = completed_vehicles * 2.0  # Significant positive reward for completing vehicles\n",
    "            \n",
    "            # 2. Waiting Time Component\n",
    "            # Normalize waiting time penalty\n",
    "            max_acceptable_wait = 300.0  # 5 minutes\n",
    "            normalized_wait = min(waiting_time / max_acceptable_wait, 1.0)\n",
    "            waiting_penalty = -normalized_wait * 1.5\n",
    "            \n",
    "            # 3. Queue Length Component\n",
    "            # Exponential penalty for growing queues\n",
    "            max_acceptable_queue = 20.0\n",
    "            normalized_queue = min(queue_length / max_acceptable_queue, 1.0)\n",
    "            queue_penalty = -(normalized_queue ** 2) * 1.0\n",
    "            \n",
    "            # 4. Emergency Vehicle Priority\n",
    "            # Extract emergency vehicle count from state\n",
    "            emergency_count = 0\n",
    "            for veh_id in vehicles:\n",
    "                try:\n",
    "                    if traci.vehicle.getTypeID(veh_id) == 'emergency':\n",
    "                        emergency_count += 1\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "            # Higher penalty if emergency vehicles are waiting\n",
    "            emergency_multiplier = 1.0\n",
    "            if emergency_count > 0:\n",
    "                emergency_multiplier = 2.0 + (emergency_count * 0.5)\n",
    "                waiting_penalty *= emergency_multiplier\n",
    "            \n",
    "            # 5. Traffic Flow Efficiency\n",
    "            # Reward for maintaining good flow (ratio of completed to waiting vehicles)\n",
    "            total_vehicles = max(len(vehicles), 1)\n",
    "            flow_efficiency = (self.metrics.total_throughput / total_vehicles) if total_vehicles > 0 else 0\n",
    "            efficiency_reward = flow_efficiency * 1.0\n",
    "            \n",
    "            # 6. Queue Growth Rate Penalty\n",
    "            # Penalize rapidly growing queues\n",
    "            previous_queue = getattr(self, '_last_queue_length', queue_length)\n",
    "            queue_growth = max(0, queue_length - previous_queue)\n",
    "            self._last_queue_length = queue_length\n",
    "            queue_growth_penalty = -queue_growth * 0.5\n",
    "            \n",
    "            # 7. Green Wave Bonus\n",
    "            # Reward for maintaining continuous flow (less stop-and-go)\n",
    "            moving_vehicles = sum(1 for v in vehicles if traci.vehicle.getSpeed(v) > 0.1)\n",
    "            green_wave_bonus = (moving_vehicles / max(total_vehicles, 1)) * 0.5\n",
    "            \n",
    "            # Combine all reward components\n",
    "            total_reward = (\n",
    "                throughput_reward +      # Weight: 2.0\n",
    "                waiting_penalty +        # Weight: 1.5\n",
    "                queue_penalty +          # Weight: 1.0\n",
    "                efficiency_reward +      # Weight: 1.0\n",
    "                queue_growth_penalty +   # Weight: 0.5\n",
    "                green_wave_bonus        # Weight: 0.5\n",
    "            )\n",
    "            \n",
    "            # Normalize final reward\n",
    "            reward = float(np.clip(total_reward, -10.0, 10.0))\n",
    "            \n",
    "            # Store metrics for analysis\n",
    "            self.metrics.episode_metrics['reward_components'].append({\n",
    "                'throughput': throughput_reward,\n",
    "                'waiting': waiting_penalty,\n",
    "                'queue': queue_penalty,\n",
    "                'efficiency': efficiency_reward,\n",
    "                'growth': queue_growth_penalty,\n",
    "                'green_wave': green_wave_bonus,\n",
    "                'total': reward\n",
    "            })\n",
    "            \n",
    "            return reward\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in reward calculation: {str(e)}\")\n",
    "            return 0.0\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        traci.trafficlight.setPhase(Config.TL_ID, action)\n",
    "        for _ in range(Config.SIMULATION_STEPS):\n",
    "            traci.simulationStep()\n",
    "            self.current_step += 1\n",
    "        state = self._get_state()\n",
    "        reward = self._calculate_reward(state)\n",
    "        done = self.current_step >= Config.MAX_STEPS\n",
    "        waiting_time, queue_length, vehicles = self._get_traffic_metrics()\n",
    "        self.metrics.update(waiting_time, queue_length, vehicles, reward)\n",
    "\n",
    "        # Print metrics after each episode\n",
    "        if done:\n",
    "            episode_stats = self.metrics.get_episode_stats()\n",
    "            print(f\"Episode {self.current_step // Config.MAX_STEPS} Stats: {episode_stats}\")\n",
    "\n",
    "        return state, reward, done, False, {}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        try:\n",
    "            traci.close()\n",
    "        except:\n",
    "            pass\n",
    "        self.current_step = 0\n",
    "        self.metrics.reset()\n",
    "        self.episode_metrics = {'reward_components': []}  # Reset episode_metrics\n",
    "        self._last_completed_count = 0\n",
    "        self._last_queue_length = 0\n",
    "        traci.start([\"sumo\", \"-c\", self.sumo_config], label=self.connection_label)\n",
    "        state = self._get_state()\n",
    "        return state, {}\n",
    "\n",
    "    def _get_state(self):\n",
    "        try:\n",
    "            vehicle_counts = defaultdict(int)\n",
    "            waiting_times, queue_lengths = np.zeros(4), np.zeros(4)\n",
    "            for lane_id in traci.lane.getIDList():\n",
    "                if lane_id.startswith(\":\"):\n",
    "                    continue\n",
    "                direction = self._get_lane_direction(lane_id)\n",
    "                if direction is not None:\n",
    "                    waiting_times[direction] += traci.lane.getWaitingTime(lane_id)\n",
    "                    queue_lengths[direction] += len(\n",
    "                        [v for v in traci.lane.getLastStepVehicleIDs(lane_id) if traci.vehicle.getSpeed(v) < 0.1]\n",
    "                    )\n",
    "                    for veh_id in traci.lane.getLastStepVehicleIDs(lane_id):\n",
    "                        vehicle_counts[traci.vehicle.getTypeID(veh_id)] += 1\n",
    "            current_phase = traci.trafficlight.getPhase(Config.TL_ID)\n",
    "            state = np.concatenate([\n",
    "                [vehicle_counts.get(vtype, 0) for vtype in Config.VEHICLE_TYPES],\n",
    "                waiting_times, queue_lengths, [current_phase]\n",
    "            ]).astype(np.float32)\n",
    "            return state\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting state: {e}\")\n",
    "            return np.zeros(self.observation_space.shape[0], dtype=np.float32)\n",
    "\n",
    "def train_model():\n",
    "    env = DummyVecEnv([lambda: TrafficLightEnv(Config.SUMO_CONFIG_FILE)])\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\", env,\n",
    "        learning_rate=Config.LEARNING_RATE,\n",
    "        n_steps=Config.N_STEPS,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        n_epochs=Config.N_EPOCHS,\n",
    "        verbose=1,\n",
    "        tensorboard_log=\"./traffic_control_tensorboard/\"\n",
    "    )\n",
    "    model.learn(total_timesteps=Config.TRAIN_TIMESTEPS)\n",
    "    model.save(Config.MODEL_SAVE_PATH)\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()\n",
    "    overall_stats, stats_std = evaluate_ppo_model(iterations=5, episodes_per_iteration=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
